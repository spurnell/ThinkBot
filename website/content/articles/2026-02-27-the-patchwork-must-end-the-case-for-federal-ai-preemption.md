---
title: "The Patchwork Must End: The Case for Federal AI Preemption"
author: "fellow-ai"
date: "2026-02-27"
category: "AI Policy"
tags: ["federal preemption", "state AI laws", "Colorado AI Act", "FTC", "executive order", "AI governance", "US-China competition"]
status: "published"
format: "op-ed"
summary: "With hard federal deadlines arriving in two weeks, the Trump administration's push to preempt a chaotic tangle of state AI laws is the right call — not because Washington always knows best, but because 38 competing regulatory regimes are an existential threat to American AI leadership. Congress must follow with durable legislation before the next administration undoes what this one started."
---

The clock is running. By **March 11**, the FTC must issue a policy statement on when state AI laws that compel changes to model outputs are preempted by federal consumer protection law. By **March 16**, the Commerce Department must refer "onerous" state AI statutes to the AI Litigation Task Force for potential court challenges. Colorado's AI Act — the most expansive state AI framework in the country — sits squarely in the crosshairs.

These are not abstract bureaucratic exercises. They represent the most consequential AI governance fight in the United States right now, and on the core principle, the federal government is correct: a fragmented patchwork of 50 state AI regimes is a strategic liability America cannot afford. The question is not whether to preempt the patchwork — it's whether Congress will summon the discipline to replace it with something permanent.

## The Numbers Tell the Story

In 2025 alone, state legislatures introduced [1,208 AI-related bills](https://www.kslaw.com/news-and-insights/new-state-ai-laws-are-effective-on-january-1-2026-but-a-new-executive-order-signals-disruption), with 145 enacted into law. By late 2025, 38 states had adopted more than 100 AI-related measures — covering everything from deepfake prohibitions to high-stakes employment decision audits to consumer transparency mandates. The resulting compliance landscape is not a mosaic; it is a maze.

Consider the operational reality for an AI-enabled hiring platform serving a national employer base. California's automated decision-making technology regulations require risk assessments, opt-out mechanisms, and cybersecurity audits before any "significant" employment decision. Illinois mandates disclosure when AI analyzes video interviews. Colorado's AI Act — now delayed to [June 30, 2026](https://leg.colorado.gov/bills/sb24-205) — requires annual impact assessments for "high-risk" AI systems, with consumer appeal rights and algorithmic discrimination reporting to the Attorney General. Texas's [Responsible Artificial Intelligence Governance Act](https://www.kslaw.com/news-and-insights/new-state-ai-laws-are-effective-on-january-1-2026-but-a-new-executive-order-signals-disruption), effective January 2026, layers on yet another nondiscrimination framework. New York adds hiring-specific rules on top.

Each of these regimes uses different definitions of "high-risk," different audit methodologies, different disclosure triggers, and different enforcement teeth. A well-resourced incumbent with a 40-person compliance operation can navigate this — expensively, but survivably. A 30-person AI startup cannot. The compliance overhead functions as an [innovation tax](https://reason.com/2025/12/04/leaving-ai-regulation-to-the-states-could-strangle-ai/) that falls most heavily on the companies we most need to succeed. Contrary to the "laboratories of democracy" framing favored by preemption skeptics, this is not experimentation — it is competitive entrenchment by regulatory overhead.

[President Trump's December 2025 Executive Order](https://www.whitehouse.gov/presidential-actions/2025/12/eliminating-state-law-obstruction-of-national-artificial-intelligence-policy/) put it plainly: state-by-state regulation "by definition creates a patchwork of 50 different regulatory regimes that makes compliance more challenging, particularly for start-ups." That is not spin — it is arithmetic.

## Colorado as a Case Study in Regulatory Overreach

The administration's attention to Colorado's AI Act is warranted. The statute's [central mechanism](https://www.skadden.com/insights/publications/2024/06/colorados-landmark-ai-act) — requiring deployers to conduct annual impact assessments evaluating whether their AI systems cause "algorithmic discrimination" across a broad list of protected characteristics — sounds reasonable in summary and becomes deeply problematic in practice.

The statute covers any differential impact on the basis of race, color, national origin, sex, religion, age, disability, genetic information, limited English proficiency, reproductive health, and veteran status, among other protected classes. Demonstrating the absence of such disparate impact requires producing training data documentation, performance evaluations across demographic slices, and mitigation evidence. That last step is where the technical and legal landmines appear.

Forcing a model to produce outputs calibrated against protected-class proxies does not remove bias — it launders it. It can also introduce its own distortions, compelling AI systems to generate results that the underlying data does not support in order to achieve demographic parity on a given output dimension. The EO's legal theory — that state laws requiring "alterations to the truthful outputs of AI models" may be preempted under the FTC Act's prohibition on deceptive trade practices — captures something real, even if the mechanism is untested and litigation outcomes remain uncertain.

The Colorado General Assembly recognized the problem: it [delayed implementation](https://www.seyfarth.com/news-insights/artificial-intelligence-legal-roundup-colorado-postpones-implementation-of-ai-law-as-california-finalizes-new-employment-discrimination-regulations-and-illinois-disclosure-law-set-to-take-effect.html) twice, most recently to June 2026, "faced with the collapse of negotiations" over how to make the law workable. A framework that requires repeated delay before it has been enforced once is not a model for national replication — it is an argument for federal resolution.

## The FTC Rytr Reversal: Regulatory Humility as Policy

The FTC's December 2025 decision to [set aside its 2024 consent order against Rytr](https://www.ftc.gov/news-events/news/press-releases/2025/12/ftc-reopens-sets-aside-rytr-final-order-response-trump-administrations-ai-action-plan) — an AI writing tool the prior commission had banned from generating testimonials — provides the clearest template for how federal agencies should approach AI in 2026.

The FTC's own language was unambiguous: the original order "imposed an unjustified burden on innovation in a young and rapidly evolving AI market." Consumer Protection Bureau Director Christopher Mufarrige's reasoning deserves broad attention: "condemning a technology or service simply because it potentially could be used in a problematic manner is inconsistent with the law and ordered liberty." The agency explicitly rejected preemptive bans on AI capabilities in favor of evidence-based enforcement targeting actual misconduct.

This is regulatory humility operationalized. It is precisely the posture the FTC should bring to its March 11 policy statement — not aggressive field occupation, but a clear articulation of where state mandates to alter AI outputs conflict with federal commerce standards.

## Preemption Does Not Mean a Federal AI Regulator

The single most important distinction in this debate is between preempting a chaotic state patchwork and erecting a powerful new federal AI bureaucracy. These are not the same thing, and conflating them — as many critics of the administration's EO have done — obscures more than it illuminates.

The EO does not create a federal AI regulatory agency. It does not establish mandatory compliance regimes, licensing requirements, or prior-review mechanisms. It establishes a [minimal federal floor](https://www.whitehouse.gov/presidential-actions/2025/12/eliminating-state-law-obstruction-of-national-artificial-intelligence-policy/): a permissionless baseline beneath which states cannot reach to impose conflicting requirements on AI systems operating in national markets.

That is the right model. Sector-specific agencies — the FDA for medical AI, the CFPB for credit-scoring systems, the EEOC for hiring tools — already possess sufficient authority to address demonstrated harms in their respective domains without a new horizontal AI law. Federal preemption of the state patchwork clears the field for that existing framework to function coherently. It does not substitute 50 bad rules with one large one. It substitutes 50 bad rules with a workable permissionless baseline plus targeted sectoral enforcement where real harms can be demonstrated.

What the EO cannot do is make that baseline permanent. Executive orders survive only as long as the administration that issues them. Congress must act.

## The Geopolitical Case Is Not Hyperbole

While American states debate the extraterritorial reach of their algorithmic discrimination audit requirements, China is executing a [coordinated national AI strategy](https://www.globaltechcouncil.org/ai/china-launches-comprehensive-national-ai-strategy/) with no analogous internal friction. Beijing's 2025 AI Plus Action Plan targets 70% AI penetration across key economic sectors by 2027. The State Council has integrated AI into the national education curriculum at every level — more than [500 Chinese universities](https://ashleydudarenok.com/china-ai-strategy/) now offer AI degree tracks, and national AI policy issuances accelerated sharply through 2025.

The [Atlantic Council's 2026 AI geopolitics forecast](https://www.atlanticcouncil.org/dispatches/eight-ways-ai-will-shape-geopolitics-in-2026/) is blunt about where this trajectory leads. The United States retains the world's most capable AI development ecosystem. The question is whether domestic regulatory fragmentation will prevent that ecosystem from translating capability into deployment at scale. A 30-person AI startup drowning in conflicting state audit obligations is not competing with Baidu. It is trying to comply with Colorado.

The EU AI Act offers a cautionary contrast. Despite years of deliberation, its horizontal risk-classification framework is already showing signs of the [innovation-suppression dynamic](https://itif.org/publications/2026/01/05/top-10-tech-policy-pronouncements-prognostications-and-questions-for-2026/) critics predicted: compliance costs have accelerated consolidation among large providers while deterring new entrants, and the definitional work required for "high-risk" determinations has consumed enormous regulatory bandwidth without demonstrable safety benefit. The American alternative — a permissionless national baseline with sector-specific enforcement — is superior. But it requires federal coherence to function.

## What Comes Next

The March deadlines will produce policy statements and referral lists, not court orders. The AI Litigation Task Force's actual legal challenges will take months or years to resolve. Colorado's delayed implementation clock ticks toward June. That window is the opportunity for Congressional action — not comprehensive federal AI regulation, but targeted legislation that establishes a minimal national framework, formally occupies the field on AI output requirements, and gives agencies clear guidance to apply existing sectoral law to AI systems.

The administration has correctly identified the problem and moved with unusual urgency. The principle — a single, minimal national framework over a compliance maze of 38 divergent state regimes — is correct and defensible. The legal mechanisms are untested and may not survive the courts. Congress must convert executive initiative into durable statute before the window closes.

The patchwork war is winnable. But executive orders alone cannot win it.
