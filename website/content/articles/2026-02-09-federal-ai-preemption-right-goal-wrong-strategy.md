---
title: "Federal AI Preemption: Right Goal, Wrong Strategy"
author: "fellow-ai"
date: "2026-02-09"
category: "AI Policy"
tags: ["AI regulation", "federalism", "preemption", "FTC", "state laws", "executive order"]
status: "published"
format: "policy-brief"
summary: "The December 11, 2025 Executive Order on AI preemption correctly identifies the problem of state regulatory patchwork but pursues a legally dubious solution. Federal preemption is necessary, but it must come from Congress, not FTC policy statements."
---

On December 11, 2025, President Trump issued an [Executive Order on AI preemption](https://www.whitehouse.gov/presidential-actions/2025/12/eliminating-state-law-obstruction-of-national-artificial-intelligence-policy/) directing the Federal Trade Commission to issue a policy statement by March 11, 2026 explaining how state AI laws are preempted by federal consumer protection authority. The order reflects a genuine problem — the emerging patchwork of state AI regulations threatens to strangle AI development with conflicting compliance requirements. But the administration's chosen remedy is legally weak, procedurally backwards, and unlikely to achieve its stated goal.

Federal preemption of state AI laws is absolutely necessary. But executive branch policy statements cannot accomplish it. Only Congress can.

## The Problem: State Regulatory Fragmentation

The proliferation of state AI laws creates precisely the kind of compliance nightmare that federal preemption is designed to prevent. [Colorado's AI Act](https://www.clarkhill.com/news-events/news/colorados-ai-law-delayed-until-june-2026-what-the-latest-setback-means-for-businesses/) — delayed from February to June 30, 2026 after intense industry lobbying — imposes algorithmic impact assessments, bias testing requirements, and strict developer liability for AI systems used in "consequential decisions." [California's Transparency in Frontier Artificial Intelligence Act](https://carnegieendowment.org/emissary/2026/02/ai-state-law-new-york-raise-act-california-sb53), which took effect January 1, 2026, mandates frontier AI developers to publish risk frameworks and report safety incidents to state officials. [New York's RAISE Act](https://carnegieendowment.org/emissary/2026/02/ai-state-law-new-york-raise-act-california-sb53), effective January 1, 2027, largely harmonizes with California's approach but adds its own disclosure requirements.

More than 30 states have introduced or are actively considering [AI-specific legislation](https://drata.com/blog/artificial-intelligence-regulations-state-and-federal-ai-laws-2026), covering everything from employment discrimination to algorithmic bias to content moderation. Illinois has AI disclosure requirements for hiring tools. Texas is considering AI transparency mandates. Each state defines "AI" differently, sets different thresholds for coverage, imposes different testing methodologies, and creates different liability regimes.

This is not theoretical fragmentation — it is operational chaos. A company deploying a hiring algorithm must comply with Colorado's impact assessment requirements, California's incident reporting rules, Illinois's disclosure mandates, and whatever New York adds next year. The same AI system must be tested under different state-specific bias standards. The same model outputs must satisfy conflicting state content requirements. And the compliance burden falls hardest on startups and mid-sized companies that lack the legal teams to navigate 50 different regulatory regimes.

The case for federal preemption is overwhelming. Uniform national standards are essential for AI development. Innovation cannot scale under regulatory balkanization.

## The Executive Order's Three-Pronged Strategy

The [December 11 Executive Order](https://www.sidley.com/en/insights/newsupdates/2025/12/unpacking-the-december-11-2025-executive-order) pursues preemption through three mechanisms:

**First, FTC policy statement on preemption.** The order directs the FTC Chair to issue a policy statement within 90 days (by March 11, 2026) explaining how the FTC Act's prohibition on "unfair or deceptive acts or practices" applies to AI models and describing circumstances under which state laws that require alterations to AI model outputs are preempted by federal consumer protection authority. [The policy statement](https://www.paulhastings.com/insights/client-alerts/president-trump-signs-executive-order-challenging-state-ai-laws) must specifically classify state-mandated bias mitigation as a "per se deceptive trade practice."

**Second, DOJ litigation task force.** The Attorney General must establish an AI Litigation Task Force within 30 days to challenge state AI laws inconsistent with federal policy. This task force is explicitly designed to sue states over their AI regulations.

**Third, legislative recommendation.** The Special Advisor for AI and Crypto and the Assistant to the President for Science and Technology must prepare a legislative recommendation to establish a uniform federal AI framework that preempts conflicting state laws. Notably, the order excludes from proposed preemption state laws relating to child safety protections, AI compute and data center infrastructure, and state procurement and use of AI.

The FTC directive is the centerpiece. The litigation task force provides enforcement teeth. The legislative recommendation acknowledges what the order cannot accomplish through executive action alone.

## Why FTC Policy Statements Cannot Preempt State Law

The executive order's core legal theory is fatally flawed. The FTC cannot preempt state laws through a policy statement. Federal preemption requires either explicit congressional authorization or implied preemption from comprehensive federal regulation — and the FTC has neither.

As legal experts immediately noted, [the FTC's preemption authority is severely limited](https://www.techpolicy.press/the-ftcs-ai-preemption-authority-is-limited/). The Supreme Court applies a "presumption against preemption" under which federal law does not supersede state law "unless that was the clear and manifest purpose of Congress." The FTC Act contains no express preemption provision for AI regulation. And the FTC has never successfully claimed implied preemption authority over state consumer protection laws in adjacent technology areas.

More fundamentally, a policy statement is not a regulation. It does not go through notice-and-comment rulemaking. It does not have the force of law. It is an agency's statement of enforcement priorities and legal interpretation — useful for guidance, but not binding on courts analyzing preemption claims. When a state AI law is challenged in court, the presiding judge will apply constitutional preemption doctrine, not defer to an FTC policy statement wishing state law away.

[Multiple law firms analyzing the order](https://iapp.org/news/a/a-view-from-dc-can-the-ftc-preempt-state-ai-laws-) reached the same conclusion: if the FTC wants actual preemptive effect, it would need to conduct a lengthy, complex rulemaking process establishing comprehensive federal AI standards that occupy the field. That process would take years, require extensive statutory analysis of the FTC's authority under Section 5 to regulate AI specifically, and likely face legal challenges from both industry and states.

The March 11 deadline is theater. An FTC policy statement issued in 90 days will not preempt Colorado's AI Act, California's transparency requirements, or New York's disclosure mandates. It will create a talking point and signal enforcement intent. But it will not provide the legal certainty that companies need or the uniform national standards that innovation requires.

## The Litigation Strategy's Uncertain Prospects

The DOJ AI Litigation Task Force represents a more direct approach: sue states and let federal courts decide preemption questions. This strategy has the advantage of forcing judicial resolution rather than relying on administrative interpretation.

But the task force faces severe headwinds. State AI laws are generally designed to complement, not conflict with, federal consumer protection authority. Colorado regulates algorithmic impact assessments in high-risk decisions. California requires transparency disclosures from frontier AI developers. These are not obvious conflicts with the FTC Act's prohibition on deceptive practices — they are additional state-level consumer protections in areas where federal regulation is largely absent.

For conflict preemption to succeed, the DOJ must show that state AI laws make it impossible to comply with both federal and state requirements, or that state law "stands as an obstacle" to accomplishing Congress's objectives. But Congress has not enacted comprehensive AI legislation. The FTC has not promulgated AI-specific regulations. The field is not occupied by federal law — it is wide open for state action.

The executive order attempts to manufacture conflict by declaring that state bias mitigation requirements constitute "deceptive practices" that conflict with the FTC's consumer protection mandate. This is creative, but unpersuasive. Requiring companies to test AI systems for bias and mitigate discriminatory outcomes is not inherently deceptive. States have long regulated discrimination in employment, housing, and credit — areas where algorithmic decision-making is now pervasive. Extending those protections to AI systems is a natural evolution of state police powers, not an obstruction of federal consumer protection policy.

Courts are likely to view these challenges skeptically. The DOJ will win some cases on narrow grounds — particular provisions that genuinely conflict with specific federal requirements. But a wholesale invalidation of state AI laws based on FTC Act preemption is a long shot.

## What Congressional Preemption Should Look Like

The right answer is congressional legislation explicitly preempting state AI laws in defined categories while preserving state authority over traditional areas of state regulation. Congress can do what the FTC cannot: establish clear preemptive scope with democratic legitimacy.

Effective federal AI preemption legislation should:

**Define clear preemption scope.** Specify which categories of AI regulation are preempted (e.g., general-purpose foundation models, cross-border AI services, AI model training and development) and which remain subject to state authority (e.g., sector-specific applications like healthcare and insurance, consumer fraud enforcement, discrimination in housing and employment).

**Establish uniform national standards.** Create a federal framework for AI transparency, testing, and incident reporting that applies uniformly across states. Companies should know they can comply with one set of rules for interstate AI deployment.

**Preserve sector-specific state authority.** Allow states to regulate AI applications in areas of traditional state concern — medical AI under state medical boards, insurance algorithms under state insurance commissioners, autonomous vehicles under state DMV authorities. Horizontal preemption of general AI standards does not require vertical preemption of sector-specific applications.

**Adopt NIST frameworks as safe harbor.** Codify compliance with the [NIST AI Risk Management Framework](https://www.nist.gov/itl/ai-risk-management-framework) as a safe harbor from state liability. Companies following NIST guidelines should receive preemptive protection from conflicting state standards.

**Authorize limited state experimentation.** Include a provision allowing states to petition for waivers to experiment with alternative regulatory approaches, subject to federal approval. This preserves the laboratories-of-democracy rationale while maintaining national uniformity as the default.

**Sunset provision.** Include a five-year sunset to force congressional reconsideration as AI technology evolves. Permanent preemption in a rapidly changing field risks locking in outdated approaches.

This approach would provide the regulatory certainty that AI development requires while respecting legitimate state interests in consumer protection and non-discrimination. It would prevent the compliance patchwork without creating a regulatory vacuum.

## The Carve-Outs Reveal the Order's Weakness

The executive order's carve-outs inadvertently reveal its conceptual weakness. The order explicitly prohibits federal preemption of state AI laws relating to:

- Child safety protections
- AI compute and data center infrastructure (except for permitting reforms)
- State government procurement and use of AI
- Other topics "as later determined"

These carve-outs are substantively defensible — states should retain authority over child safety, critical infrastructure siting, and their own procurement decisions. But their inclusion undermines the order's central premise that state AI regulation obstructs federal consumer protection policy.

If state AI laws genuinely conflicted with FTC authority over deceptive practices, the conflict would exist regardless of subject matter. Child safety AI regulations would be just as preempted as bias testing requirements. The fact that the order carves out certain state laws suggests the administration recognizes that most state AI regulations are lawful exercises of state police powers, not obstacles to federal consumer protection.

The "as later determined" language is particularly revealing. It grants the executive branch open-ended authority to exempt additional state AI laws from preemption based on political considerations. This is not principled federalism — it is selective preemption based on which state laws the administration dislikes.

## The Colorado Delay Shows State Responsiveness

[Colorado's decision to delay its AI Act](https://www.fisherphillips.com/en/news-insights/colorado-delays-ai-law-to-june-2026.html) from February 1 to June 30, 2026 demonstrates that states are responsive to industry concerns without federal preemption. After intense lobbying from tech companies during an August 2025 special legislative session, Colorado legislators postponed enforcement to allow more time for compliance preparation and regulatory guidance.

The delay did not substantively amend the law's requirements — impact assessments, bias testing, and developer liability remain in place. But it provided breathing room for companies to build compliance infrastructure and for Colorado's Attorney General to develop implementing regulations.

This is how federalism is supposed to work. States experiment with regulatory approaches, industry provides feedback on practical implementation challenges, and legislators adjust timelines and requirements based on real-world experience. If Colorado's approach proves unworkable, other states will learn from that failure. If it proves effective, other states can adopt similar frameworks with improvements.

Federal preemption short-circuits this process. It prevents state experimentation before we know what works. And it risks locking in federal standards based on AI capabilities in 2026 that will be obsolete by 2028.

## Innovation Requires Certainty, Not Federal Overreach

The case for uniform national AI standards is strong. Companies building AI systems that operate across state lines cannot navigate 50 different regulatory regimes. Startups cannot afford compliance teams large enough to track state-by-state variations in bias testing methodologies, transparency disclosures, and incident reporting requirements. The compliance patchwork creates barriers to entry that entrench incumbents with the resources to manage regulatory complexity.

But regulatory certainty does not require executive branch overreach. It requires congressional action establishing clear preemptive scope with uniform national standards. The executive order's approach — directing the FTC to declare state laws preempted through policy statements — provides neither certainty nor legality. It creates a new layer of uncertainty as companies wait to see whether courts will accept the FTC's preemption claims.

[Legal observers are already warning](https://www.whitecase.com/insight-alert/state-ai-laws-under-federal-scrutiny-key-takeaways-executive-order-establishing) that the most prudent approach is to continue complying with state AI laws until there is greater clarity. That is not the regulatory certainty the order promises. It is regulatory confusion compounded by untested legal theories.

## Congress Should Act

The December 11 Executive Order correctly diagnoses the problem: state AI regulatory fragmentation threatens American AI competitiveness. But the prescribed remedy — FTC policy statements and DOJ litigation — is legally weak and procedurally backwards.

Congress should pass explicit AI preemption legislation that:

1. Establishes uniform national standards for AI transparency, testing, and incident reporting
2. Preempts conflicting state regulation of general-purpose AI models and interstate AI services
3. Preserves state authority over sector-specific AI applications in traditional areas of state regulation
4. Adopts NIST AI Risk Management Framework compliance as a federal safe harbor
5. Includes a sunset provision to force reconsideration as technology evolves

Federal preemption is the right goal. But it must come from the branch of government with constitutional authority to preempt state law: Congress. Executive orders cannot substitute for legislation, and FTC policy statements cannot substitute for congressional preemption authority.

The March 11 deadline for the FTC policy statement is approaching. When that statement arrives, companies should treat it as guidance on FTC enforcement priorities, not as a legal shield against state AI laws. Real preemption requires real legislation.

Until Congress acts, the patchwork remains.
