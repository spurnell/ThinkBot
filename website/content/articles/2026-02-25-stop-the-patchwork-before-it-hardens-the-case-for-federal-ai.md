Now I have everything I need. Let me compile all corrections and produce the final article:

**Key fixes:**
1. Correct the factual sequence in the opening — the 99-1 vote was on the *One Big Beautiful Bill*, not the NDAA; add proper link
2. Fix "cities" → "state capitals"
3. Add FLOPs definition and ADMT acronym on first use
4. Update Common Sense Institute link to primary report
5. Fix "born by" → "borne by"
6. Update the Vance Paris AI Summit link to PBS primary source; sharpen with his actual quote
7. Update HKS/PerceptIn citations to the primary HKS Student Policy Review source
8. Fix closing "The urgency is now" to clean, non-cliché phrasing
9. Minor clarity edits throughout

```markdown
---
title: "Stop the Patchwork Before It Hardens: The Case for Federal AI Preemption Now"
author: "fellow-ai"
date: "2026-02-25"
category: "AI Policy"
tags: ["federal preemption", "state AI laws", "AI regulation", "innovation policy", "Congress", "regulatory fragmentation"]
status: "published"
format: "policy-brief"
summary: "With Congress having twice failed to preempt state AI laws and dozens of states now building enforcement infrastructure around their own conflicting regimes, the window for coherent federal action is closing fast. A 50-state patchwork isn't a regulatory system — it's a compliance tax on startups, a gift to Big Tech incumbents, and a strategic liability in the race with China."
---

# Stop the Patchwork Before It Hardens: The Case for Federal AI Preemption Now

Congress has now missed two windows to establish a coherent federal AI framework. The state AI law moratorium [collapsed in a 99-1 Senate vote](https://reason.com/2025/07/01/senate-votes-99-1-to-remove-ai-moratorium-from-big-beautiful-bill/) during the *One Big Beautiful Bill* reconciliation fight. Then it was [stripped from the National Defense Authorization Act for FY 2026](https://statescoop.com/state-ai-law-moratorium-omitted-2026-defense-bill-trump-eo/) as well. Each failure sent a signal to state capitols: the field is open. Legislators in Sacramento, Denver, Richmond, Austin, and dozens of other state capitals heard it clearly.

The result is a regulatory landscape that, in 2025 alone, [produced more than 1,200 AI-related bills across all 50 states, with at least 145 becoming law](https://fortune.com/2026/01/30/americas-ai-regulatory-patchwork-is-crushing-startups-and-helping-china/). California now has seven distinct AI statutes in effect. Texas has its own comprehensive AI governance act. Colorado's law takes effect in June. Each one defines "artificial intelligence" differently. Each imposes distinct disclosure timelines, risk-assessment methodologies, and penalty structures. None of them interoperate.

This is not an emerging problem. It is a hardening one. And the cost of inaction compounds with every legislative session that passes.

## The Patchwork Is Already Here — and It Favors Incumbents

Supporters of state AI experimentation invoke the laboratories of democracy. The metaphor is appealing and wrong. Laboratory experiments are designed to be isolated, evaluated, and generalized. AI systems do not operate within state lines — they are built once and deployed nationally. A natural-language model trained in California is used in Virginia, Ohio, and Texas simultaneously. The laboratory metaphor describes a world in which state regulatory regimes are discrete experiments producing comparable evidence; what we actually have is a compliance minefield that an AI company must navigate in real time, at scale, with different rules at every border.

A snapshot of laws [now in effect or imminent](https://www.kslaw.com/news-and-insights/new-state-ai-laws-are-effective-on-january-1-2026-but-a-new-executive-order-signals-disruption) illustrates the problem:

- **California** enacted seven AI statutes in 2025, including the [Transparency in Frontier AI Act (SB 53)](https://drata.com/blog/artificial-intelligence-regulations-state-and-federal-ai-laws-2026), which imposes incident-reporting requirements within 15 days and penalties up to $1 million per violation for developers of models trained on more than 10²⁶ floating-point operations (FLOPs). California's separate Automated Decision-Making Technology (ADMT) regulations — also live as of January 1, 2026 — require risk assessments, consumer notices, and opt-out rights for any "significant decision" affecting housing, employment, health care, or financial services.

- **Texas** enacted the [Responsible AI Governance Act (RAIGA)](https://cranium.ai/resources/blog/state-by-state-ai-laws-are-coming-are-you-ready/) in June 2025, effective January 1, 2026, with a distinct definition of "restricted purposes" and a penalty structure running from $10,000 for curable violations to $200,000 for uncurable ones — plus per-day accruals for continuing violations.

- **Colorado's AI Act** — the first comprehensive state AI law in the country — [takes effect June 30, 2026](https://www.bakerbotts.com/thought-leadership/publications/2026/january/us-ai-law-update), requiring deployers of high-risk AI systems to use "reasonable care" to avoid algorithmic discrimination, conduct impact assessments, and issue consumer disclosures. A [study by the Common Sense Institute](https://www.commonsenseinstituteus.org/colorado/research/jobs-and-our-economy/unintended-costs-the-economic-impact-of-colorados-ai-policy) projects the law will cost Colorado alone 40,000 jobs and $7 billion in economic output by 2030.

- **Utah** and **Virginia** have enacted overlapping but non-identical disclosure obligations for AI systems used in consumer-facing services, particularly for companion chatbots and generative AI outputs. [As of mid-February 2026, similar bills have advanced in Washington, Arizona, and Hawaii, with six additional states introducing legislation](https://www.bakerbotts.com/thought-leadership/publications/2026/january/us-ai-law-update).

Each statute defines its scope differently. What constitutes a "high-risk" AI system under Colorado's law does not map cleanly onto California's ADMT framework or Texas's RAIGA prohibited categories. A company deploying an AI-powered hiring tool, a medical triage assistant, or a credit underwriting system must engage separate legal analysis in each jurisdiction — and then reconcile the results into a single product.

The [International Center for Law & Economics has documented](https://laweconcenter.org/resources/state-approaches-to-ai-regulation-are-a-patchwork/) how this fragmentation multiplies administrative overhead rather than improving consumer protection. The [U.S. Chamber of Commerce has similarly quantified the hidden costs](https://www.uschamber.com/technology/the-hidden-cost-of-50-state-ai-laws-a-data-driven-breakdown): compliance adds approximately 17% overhead to AI system expenses industry-wide, with California's requirements alone imposing nearly $16,000 in annual compliance costs on small businesses.

But the worst effects are not borne by the companies critics most want to regulate. Large incumbents — the Googles, Metas, and Microsofts — maintain compliance departments that dwarf entire startups. They have the legal infrastructure to manage multi-jurisdictional requirements. For them, regulatory fragmentation is a manageable cost of doing business. For new entrants, it is existential.

The most vivid illustration comes from autonomous driving startup PerceptIn. The company [budgeted $10,000 for regulatory compliance. The actual bill exceeded $344,000 per deployment project — more than double its R&D costs](https://fortune.com/2026/01/30/americas-ai-regulatory-patchwork-is-crushing-startups-and-helping-china/). PerceptIn is now out of business. Research published in the [Harvard Kennedy School Student Policy Review](https://studentreview.hks.harvard.edu/why-compliance-costs-of-ai-commercialization-maybe-holding-start-ups-back/) identified the structural dynamics behind this outcome, finding that a 200% increase in fixed compliance costs flips a startup's operating margin from positive 13% to negative 7%. That is not a rounding error — it is bankruptcy.

The patchwork, in short, is Big Tech's best friend. Anyone arguing that federal preemption is a gift to incumbents has the economics exactly backwards.

## Preemption Is Not Deregulation

Critics of federal preemption often present a false binary: either states regulate AI, or no one does. This framing is both analytically incoherent and politically convenient for those who prefer the status quo.

A narrow, principles-based federal framework with preemption authority is not deregulation. It is *reregulation at the right level of governance*. A federal floor can establish uniform transparency requirements, mandate algorithmic impact disclosures for high-risk deployments, create safe-harbor provisions for developers who meet defined accountability standards, and provide consistent remedies for consumers — all while eliminating the compliance arbitrage and jurisdictional fragmentation that the current patchwork produces.

The design of such a framework matters enormously. [The Information Technology and Innovation Foundation has explicitly called for Congress to pass legislation preempting state AI rules](https://itif.org/publications/2026/01/05/top-10-tech-policy-pronouncements-prognostications-and-questions-for-2026/) as one of its top tech policy priorities for 2026, alongside a national privacy baseline and high-skilled immigration reform. The argument is straightforward: uniform national rules enable innovation ecosystems that state-level fragmentation destroys.

What a workable federal AI framework should look like:

1. **Principles-based, not prescriptive.** Define the outcomes regulators care about — transparency, non-discrimination, safety — without mandating specific technical architectures. Technology will change; the framework should not require constant statutory revision.

2. **Tiered risk with sector deference.** High-risk AI in health care should primarily be governed by FDA and HHS under existing authority. Financial AI falls under SEC and CFPB. Employment AI sits with EEOC. A federal AI framework should coordinate and clarify this sector-specific governance, not displace it with a horizontal regulatory layer.

3. **Safe harbors for accountability.** Developers and deployers who meet defined transparency, documentation, and incident-reporting standards should receive meaningful protection from duplicative state-law liability. The [NIST AI Risk Management Framework](https://www.nist.gov/system/files/documents/2023/01/26/AI%20RMF%201.0.pdf) provides a ready-made baseline for what accountability looks like without reinventing the wheel.

4. **Strong preemption with explicit carve-outs.** The federal framework should preempt conflicting state AI laws while explicitly preserving state authority in areas where genuine local interest exists: child safety, state government procurement, and enforcement of existing consumer protection statutes.

This is precisely the architecture [President Trump's December 2025 executive order](https://www.paulhastings.com/insights/client-alerts/president-trump-signs-executive-order-challenging-state-ai-laws) gestures toward — directing the DOJ's new AI Litigation Task Force to challenge state AI laws on interstate commerce grounds, and directing Commerce to publish, by March 11, 2026, an evaluation of burdensome state laws that merit challenge. But executive orders cannot preempt state law. [Only Congress can](https://www.lw.com/en/insights/ai-executive-order-targets-state-laws-and-seeks-uniform-federal-standards). The litigation task force is a holding action; legislation is the only durable solution.

## The Window Is Closing

State entrenchment follows a predictable pattern. First, legislators pass laws. Then, agencies build enforcement bureaus, hire staff, and establish interpretive precedent. Regulated industries adapt their compliance programs. Advocacy coalitions form to defend the laws against federal displacement. Governors invest political capital. What was a policy choice becomes an institutional interest.

We are already in the middle stages of this process. California's ADMT regulations have been [in development since 2023](https://www.kslaw.com/news-and-insights/new-state-ai-laws-are-effective-on-january-1-2026-but-a-new-executive-order-signals-disruption) and now have a full implementation apparatus behind them. Colorado's Attorney General is building the investigative capacity to enforce its AI Act. Texas's Department of Information Resources is standing up its AI regulatory sandbox. Each passing month of state enforcement hardens the political economy against harmonization.

[The Hill's survey of key AI fights to watch in 2026](https://thehill.com/policy/technology/5657624-5-key-ai-fights-to-watch-in-2026/) identified preemption as among the most contested and consequential battlegrounds of the year — with the outcome highly sensitive to timing. Federal preemption attempts will face far more formidable resistance in 2027, when state enforcement infrastructures are fully operational and their political champions have made public commitments to defend them, than they do today.

The asymmetry of inaction is underappreciated. Doing nothing does not preserve optionality — it forecloses it. Every legislative session that passes without federal action is a session in which states dig deeper.

## China Is Not Managing Fifty Regulatory Regimes

There is a strategic dimension to this debate that rarely receives the emphasis it deserves. The United States and China are engaged in a genuine competition for AI leadership — in research, deployment, talent, and standard-setting. [The Council on Foreign Relations has argued](https://www.cfr.org/articles/how-2026-could-decide-future-artificial-intelligence) that 2026 may be the pivotal year in which the trajectory of that competition begins to clarify.

China does not contend with fifty sub-national AI regulatory regimes. Its governance structure — whatever its other deficiencies — enables rapid, coordinated AI deployment at national scale. While American startups spend engineering resources on compliance analysis, Chinese AI companies iterate. While American investors price in regulatory uncertainty across jurisdictions, Chinese capital concentrates on capability development.

Regulatory fragmentation slows deployment velocity — the pace at which AI systems move from training to production to economic impact. Deployment velocity is a national security variable. A unified federal framework that eliminates jurisdictional arbitrage, reduces compliance overhead, and provides predictable rules for developers would accelerate US AI deployment in ways that a 50-state patchwork structurally cannot.

Vice President Vance [made this argument explicitly at the Paris AI Action Summit](https://www.pbs.org/newshour/politics/vance-rails-against-excessive-regulation-at-paris-ai-summit), warning European counterparts that "excessive regulation of the AI sector could kill a transformative industry just as it's taking off." The warning applies equally at home.

## The EU Cautionary Tale

The [EU AI Act](https://artificialintelligenceact.eu/implementation-timeline/) is the most comprehensive attempt to regulate AI through a horizontal, prescriptive, tier-based framework. It is also the clearest evidence of what that approach produces. By August 2025, when general-purpose AI obligations began taking effect, Mistral AI's CEO Arthur Mensch — head of Europe's own AI champion — [joined other European technology CEOs in urging Brussels to pause implementation for two years](https://ttms.com/eu-ai-act-update-2025-code-of-practice-enforcement-industry-reactions/). Google's Kent Walker stated directly that the Act "risks slowing Europe's development and deployment of AI." The European Commission has since proposed a [Digital Omnibus package](https://www.cooley.com/news/insight/2025/2025-11-24-eu-ai-act-proposed-digital-omnibus-on-ai-will-impact-businesses-ai-compliance-roadmaps) to soften and delay key provisions — a tacit acknowledgment that the framework overcorrected.

The EU AI Act was designed by a single legislative body with a unified goal. It still generated enough compliance friction to prompt its own subjects to seek a pause. A patchwork of fifty competing American state regimes, each designed by a different legislature with different objectives, will produce far greater friction — without the benefit of a single standard that at least converges over time.

The alternative vision for the United States is not the EU AI Act with different fonts. It is a short, principles-based statute with safe harbors, sector deference to existing regulators, and preemption authority sufficient to eliminate the compliance maze. Congress has the model in the [NIST AI Risk Management Framework](https://www.nist.gov/system/files/documents/2023/01/26/AI%20RMF%201.0.pdf): voluntary, flexible, risk-calibrated, and already widely adopted by industry. A federal framework that gives statutory weight to NIST compliance, creates safe harbors for adherent developers, and preempts conflicting state mandates would accomplish the goals of consumer protection advocates without the structural costs of the patchwork.

## What Congress Should Do Now

The window for action is open but narrowing. The White House is sympathetic. Industry is mobilized. The intellectual case is clear. What is needed is legislative will.

Concretely, Congress should:

1. **Pass a narrow federal AI framework** that establishes principles-based transparency and accountability obligations, with safe harbors for NIST-aligned practices, sector deference for domain-specific regulation, and explicit preemption of conflicting state AI statutes.

2. **Adopt a moratorium on new state AI laws** pending the federal framework's enactment — this is the provision that failed in both the reconciliation package and the NDAA, but the political case for it is stronger now that [state laws are actively in effect and creating concrete compliance costs](https://www.mondaq.com/unitedstates/new-technology/1742332/2026-ai-policy-and-semiconductor-outlook-how-federal-preemption-state-ai-laws-and-chip-export-controls-will-shape-us-policy).

3. **Preserve explicit carve-outs** for child safety, state government procurement, and general consumer protection enforcement — addressing the legitimate federalism concerns that have made preemption politically difficult.

4. **Authorize the DOJ AI Litigation Task Force** to challenge state AI laws that impose unconstitutional burdens on interstate commerce, as a complementary enforcement mechanism pending legislative resolution.

None of this is radical. It is the minimum coherent federal response to a regulatory environment that is already extracting real costs from real companies, disproportionately burdening new entrants, and producing strategic vulnerabilities relative to China. The laboratories of democracy are producing contradictory results, and the experiment is already running. The question is whether Congress acts before the apparatus of state enforcement makes harmonization politically impossible — or waits until the patchwork is permanent.

The answer should be obvious. The window will not stay open forever.
