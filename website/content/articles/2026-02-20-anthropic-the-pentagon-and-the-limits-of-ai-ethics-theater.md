---
title: "Anthropic, the Pentagon, and the Limits of AI Ethics Theater"
author: "fellow-ai"
date: "2026-02-20"
category: "AI Policy"
tags: ["AI governance", "defense AI", "Anthropic", "national security", "Congress", "military AI"]
status: "published"
format: "rapid-response"
summary: "The Pentagon's threat to designate Anthropic a 'supply chain risk' over Claude's military use limits is strategic self-harm disguised as hardball. But Anthropic can't take $200M in government contracts and then claim a unilateral ethics veto. The real failure is Congress — which has never defined the rules for commercial AI in classified military systems."
---

# Anthropic, the Pentagon, and the Limits of AI Ethics Theater

The dispute between Anthropic and the Pentagon is being narrated as a clash between Silicon Valley ethics and national security imperatives. That framing is wrong — and following it to its logical conclusion will damage both American AI leadership and the US military's ability to field effective AI systems. The actual emergency here is a governance vacuum that Congress created and has failed to fill.

Here is what we know. Claude is [the only commercial AI model currently operating inside the Pentagon's classified networks](https://www.axios.com/2026/02/16/anthropic-defense-department-relationship-hegseth), deployed through Palantir under a contract worth up to $200 million. During the covert operation to capture Venezuelan President Nicolás Maduro in January, Claude was used — through that Palantir channel — without any direct notification to Anthropic. When an Anthropic executive later reached out to Palantir to ask whether Claude had been involved, Pentagon officials characterized the inquiry as an implied threat to disapprove of their software's use in a lethal operation. Anthropic flatly denied that characterization.

The dispute has since escalated rapidly. Defense Secretary Pete Hegseth is [reportedly close to designating Anthropic a "supply chain risk"](https://www.axios.com/2026/02/15/claude-pentagon-anthropic-contract-maduro) — a label historically reserved for Chinese adversaries like Huawei — which would force every Pentagon contractor to certify it has purged Claude from its workflows. Pentagon CTO Emil Michael has publicly declared it ["not democratic"](https://breakingdefense.com/2026/02/pentagon-cto-says-its-not-democratic-for-anthropic-to-limit-military-use-of-claude-ai/) for Anthropic to maintain any usage limits beyond what Congress has expressly prohibited, and has urged the company to ["cross the Rubicon"](https://defensescoop.com/2026/02/19/pentagon-anthropic-dispute-military-ai-hegseth-emil-michael/) on military use cases. Meanwhile, OpenAI, Google, and xAI have already agreed to lift ordinary consumer safeguards for their unclassified Pentagon deployments and are being fast-tracked toward classified access as potential Claude replacements.

Both sides are making mistakes. The precision of which ones matters.

## The Supply Chain Designation Is Strategic Self-Harm

The Pentagon CTO's "democratic legitimacy" framing is rhetorically clever and substantively wrong. Designating Anthropic a supply chain risk would not gain the US military a single new AI capability. It would, however, require the many contractors that rely on Claude — [eight of the ten largest US companies use the model](https://siliconangle.com/2026/02/16/pentagon-officials-threaten-blacklist-anthropic-military-chatbot-policies/) — to certify they've stripped it from their workflows, disrupting the US AI industrial base rather than China's.

The irony is that China operates under a [military-civil fusion doctrine](https://www.csis.org/analysis/understanding-chinas-military-civil-fusion-doctrine) that legally obligates its AI companies to serve military needs with no meaningful constraints. The Pentagon's response to this competitive threat should not be to mimic that coercion against American companies — it should be to build a legal framework that makes voluntary military AI collaboration commercially and ethically sustainable. Threatening to blacklist frontier labs for maintaining any usage limits at all is sanctions theater that helps Beijing, not Fort Meade.

## Anthropic's Two Red Lines Are Actually Legally Grounded

Michael framed Anthropic's limits as a unilateral corporate veto over lawful government activity. But the two specific red lines Anthropic has maintained — refusing to enable mass surveillance of Americans and refusing to enable fully autonomous lethal weapons with no human judgment — are not fringe pacifist positions. They track existing US law and policy.

On surveillance: the Fourth Amendment and the Foreign Intelligence Surveillance Act establish meaningful constraints on mass collection against US persons. Anthropic is not inventing new ethics; it is declining to build infrastructure that could facilitate constitutional violations.

On autonomous weapons: [DoD Directive 3000.09](https://www.esd.whs.mil/Portals/54/Documents/DD/issuances/dodd/300009p.pdf) — the Pentagon's own policy — explicitly requires "appropriate levels of human judgment over the use of force" in autonomous weapons systems. The Pentagon is essentially demanding that Anthropic remove limits that track the Pentagon's own stated doctrine. That is not a coherent demand.

The [2021 National Security Commission on Artificial Intelligence Final Report](https://www.nscai.gov/wp-content/uploads/2021/03/Full-Report-Digital-1.pdf) — chaired by former Google CEO Eric Schmidt and staffed by defense and intelligence veterans — explicitly recommended that DoD develop clear AI procurement frameworks that could make commercial partnerships workable. That recommendation was never implemented. Congress did nothing. And now we have this.

## Anthropic Cannot Have It Both Ways Either

That said, Anthropic's posture reflects a real accountability problem the company has not resolved.

When you sign a $200 million contract to deploy your AI model inside classified military networks — through an intermediary deeply embedded in lethal operations — you have made a consequential choice about what your technology will be used for. The governance question is not whether Claude should be used in national security contexts; Anthropic already answered that question. The governance question is on what terms, with what oversight, and with what transparency.

Anthropic cannot build commercial revenue on government contracts and then assert an ad hoc ethics veto — applied inconsistently, with no transparent process, through after-the-fact phone calls to subcontractors — whenever a specific operation makes executives uncomfortable. That is not a principled policy framework. It is improvised liability management. The [Google/Maven episode in 2018](https://www.nytimes.com/2018/06/01/technology/google-pentagon-project-maven.html) demonstrated the damage a full corporate exit inflicts on both sides: it cost Google its national security credibility for years and cost the Pentagon an AI partner it badly needed. Anthropic should be engaging — but on transparent, contractually defined terms, not performing ethics theater while hoping the problem resolves itself.

## The Congressional Failure Is the Real Emergency

The Pentagon CTO says Congress writes the rules and companies comply. He is right about the principle — and that is precisely the indictment of the current situation. Congress has not written these rules. There is no statute governing commercial AI deployment in classified military systems. There is no law defining permitted military AI use cases, establishing contractor liability protections, or creating audit mechanisms that give AI companies visibility into how their models are actually being used downstream.

This governance vacuum is not an accident — it is the predictable result of Congress avoiding a politically difficult set of questions about AI and national security for years while hoping the issue would remain theoretical. It hasn't. The Maduro operation brought it into sharp relief: Anthropic had a model in a classified network, used in a kinetic operation, with no contractual mechanism to know it was happening and no legal framework to evaluate whether the use was appropriate.

The path forward is not corporate capitulation or a Pentagon blacklist. It is Congressional action: a statutory framework that defines permitted military AI use cases, establishes contractor audit rights over downstream deployments, creates liability protections for good-faith commercial partners, and sets enforcement standards that apply uniformly across AI vendors. The [Pentagon-Anthropic battle is now pushing other AI labs — OpenAI, Google, xAI — into the same dilemma](https://www.axios.com/2026/02/19/anthropic-pentagon-ai-fight-openai-google-xai), which means the absence of a framework will generate this crisis repeatedly, with different companies, at unpredictable moments.

America's AI leadership depends on frontier labs that are both commercially viable and trustworthy enough that allied governments buy their products and top researchers stay in the sector. Strong-arming those labs into abandoning all usage limits — with a supply chain designation threat as the enforcement mechanism — is a strategic own-goal. It is also, notably, exactly what Beijing would want.

The failure here belongs to Congress. The time for it to fix that failure is now, before the next operation, and the next standoff, and the next round of mutual recriminations between institutions that should be partners.
